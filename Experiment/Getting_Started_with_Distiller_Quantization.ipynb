{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Getting Started with Distiller Quantization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgBIPUy3lMxW"
      },
      "source": [
        "**Distiller Installation on Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf6lphwBlOfy",
        "outputId": "ddb57e77-f60d-4949-aa7a-ba66e569b104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "ROOT=\"/content/drive\"\n",
        "print(ROOT)\n",
        "drive.mount(ROOT)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VFtYf-5jsWB"
      },
      "source": [
        "Clone the git repo on your drive\n",
        "!git clone https://github.com/NervanaSystems/distiller.git\n",
        "\n",
        "I had created a folder named 'distiller' already. Realized later that it was of no need. \n",
        "So you would probably require this: %cd drive/My Drive/GE Edge Capstone 2020/distiller.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQBlUXvqXISp",
        "outputId": "5ee7d4aa-2563-416c-9037-6a68a316e76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My Drive/GE Edge Capstone 2020/distiller/distiller"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/GE Edge Capstone 2020/distiller/distiller\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t_5C9hRaH1I"
      },
      "source": [
        "!pip3 install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ_Lw5YWbjID"
      },
      "source": [
        "#Checking if Distiller is installed\n",
        "import distiller"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNZdcSwPbwxE",
        "outputId": "3053c859-5843-411f-c64f-925c45e08f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGFG3escboIb",
        "outputId": "cdcbee79-eadb-42d3-eecd-df04a6d9744a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd drive/My Drive/GE Edge Capstone 2020/distiller/distiller/examples/classifier_compression"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/GE Edge Capstone 2020/distiller/distiller/examples/classifier_compression\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkfnx19Z9cJ"
      },
      "source": [
        "**Loading the CIFAR-10 Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-0UFKR3Z46S",
        "outputId": "12f6c16f-27b6-4228-919e-85f88c85f434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_J1L1IgaBvr",
        "outputId": "22a38ea3-620a-4626-9160-8bfabada3bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset_CIFER = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrainloader_CIFER = torch.utils.data.DataLoader(trainset_CIFER, batch_size=4,\\n                                          shuffle=True, num_workers=2)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ESweGH-b3YN"
      },
      "source": [
        "#Cites all the Arguments which could be passed\n",
        "!python3 compress_classifier.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8COy2vbYvhu"
      },
      "source": [
        "**Post Quantization Training** \n",
        "\n",
        "A list of architecture is defined for a specific dataset under the 'models' folder. Currently Distiller only supports Imagenet, CIFAR-10 and MNIST datasets. Imagenet however is not publicly available so loading the dataset is tedious. In order to experiment with architectures and datasets beyond distiller we would have to modify the codes, starting with 'models' and 'dataset loaders'. 'Modules' for distiller have also been defined to enable us to create our own architectures as per distiller (slight variation from pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuheZ9EBdbo5"
      },
      "source": [
        "#Removed the pretrained argument which was given in the original command as received an error that the particular pretrained model is not available\n",
        "!python3 compress_classifier.py -a resnet20_cifar './data'  --quantize-eval --evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7-W_DFifx-Z"
      },
      "source": [
        "#Removed the pretrained argument\n",
        "!python3 compress_classifier.py -a plain20_cifar './data'  --quantize-eval --evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E5k3gVzYlsS"
      },
      "source": [
        "**Quantization Aware Training**\n",
        "\n",
        "'compress_classifier.py' is the common file used across all image classification problems. Different quantization techniques are applied through the compression scheduler which is saved as .yaml files. Besides varying the arguments from the command line, we can also make edits to the compression scheduler i.e. '.yaml' files, such as 'bits_activation', 'weights_activation', 'signed or unsigned weights' etc. This varies across schedulers. \n",
        "\n",
        "Path to access different compression schedulers built for QAT:\n",
        "Capstone Project/distiller/examples/quantization/quant_aware_train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H54bYFTdBnc"
      },
      "source": [
        "# 1. Quant_aware_train_linear_quant.yaml \n",
        "# Read more at : \"https://nervanasystems.github.io/distiller/algo_quantization.html\"\n",
        "#Removed the argument 'pretrained'\n",
        "!python compress_classifier.py -a resnet20_cifar -p 50 -b 256 './data' --epochs 10 --compress=../quantization/quant_aware_train/quant_aware_train_linear_quant.yaml -j 22 --lr 0.0001 --vs 0 --gpu 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzc2qJaecF3V"
      },
      "source": [
        "# 2. preact_resnet_cifar_dorefa.yaml\n",
        "#Pre-activation ResNet20 on CIFAR10 (base FP32 + DoReFa)\n",
        "#Changed epochs to 10 from 200\n",
        "!python compress_classifier.py -a preact_resnet20_cifar --lr 0.1 -p 50 -b 128 './data' -j 1 --epochs 10 --compress=../quantization/quant_aware_train/preact_resnet_cifar_dorefa.yaml --wd=0.0002 --vs=0 --gpus 0"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}