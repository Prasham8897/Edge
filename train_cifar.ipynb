{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_cifar.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMmclMqBBSyAbLTvamoSjhq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0kYfYIYfA5X1"},"source":["# **GIT SETUP**"]},{"cell_type":"code","metadata":{"id":"YqpvV7WFA862","executionInfo":{"status":"ok","timestamp":1602274734573,"user_tz":240,"elapsed":21000,"user":{"displayName":"PRITAM BISWAS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghgme2jepJFgS5Fic6a7DNk--0D-oBF0HK4BP8_jw=s64","userId":"11229082670386400890"}},"outputId":"663a8d75-4d08-4b19-d760-bc94d06a1e24","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BWUeUtvmBGcN"},"source":["**Change to your custom path**"]},{"cell_type":"code","metadata":{"id":"JrxJuOP8BTw_"},"source":["MY_GOOGLE_DRIVE_PATH = 'My Drive/Capstone_Pritam/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4oGpIiWBZbT"},"source":["**Setup directories**"]},{"cell_type":"code","metadata":{"id":"I9beMmNvBFIP","executionInfo":{"status":"ok","timestamp":1602274838856,"user_tz":240,"elapsed":406,"user":{"displayName":"PRITAM BISWAS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghgme2jepJFgS5Fic6a7DNk--0D-oBF0HK4BP8_jw=s64","userId":"11229082670386400890"}},"outputId":"57c0bb54-d256-4411-d295-90b29bdb8275","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["\n","GIT_TOKEN = '4f61cb936944e66abd13e7e809e199ee3a378fae'\n","GIT_USERNAME = 'mohitgulla' \n","GIT_REPOSITORY = 'Edge'\n","\n","from os.path import join  \n","\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","# It's good to print out the value if you are not sure \n","print(\"PROJECT_PATH: \", PROJECT_PATH)   \n","\n","# # In case we haven't created the folder already; we will create a folder in the project path \n","!mkdir -p \"{PROJECT_PATH}\"    \n","\n","#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n","GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n","print(\"GIT_PATH: \", GIT_PATH)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PROJECT_PATH:  /content/drive/My Drive/Capstone_Pritam/\n","GIT_PATH:  https://4f61cb936944e66abd13e7e809e199ee3a378fae@github.com/mohitgulla/Edge.git\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oNxHBriQBmew"},"source":["**Change dirs and clone repo (if needed !)**"]},{"cell_type":"code","metadata":{"id":"fe1g4X9XBdOe","executionInfo":{"status":"ok","timestamp":1602274901027,"user_tz":240,"elapsed":381,"user":{"displayName":"PRITAM BISWAS","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghgme2jepJFgS5Fic6a7DNk--0D-oBF0HK4BP8_jw=s64","userId":"11229082670386400890"}},"outputId":"482f8083-e456-44dd-9bb6-7759e98ae739","colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["%cd \"{PROJECT_PATH}\"\n","#!git clone -b Pritam \"{GIT_PATH}\"\n","%cd \"Edge\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Capstone_Pritam\n","/content/drive/My Drive/Capstone_Pritam/Edge\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c3rr31yXB4P2"},"source":["# **Model = DNN, Data = CIFAR data**"]},{"cell_type":"code","metadata":{"id":"yxehL6eeB5kE"},"source":["# %%writefile train_cifar.py\n","import time\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","\n","from model.dnn import DenseNeuralNet\n","from model.cnn import CNN\n","from data.churn_data import ChurnDataset\n","from data.telescope_data import TelescopeDataset\n","from utils.util_functions import *\n","from data.mv_data import MVDataset\n","from tqdm.auto import trange, tqdm\n","from tqdm import trange\n","from torch.utils.data import Subset\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","def split_data(dataset, val_split=0.25):\n","    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n","    datasets = {}\n","    datasets['train'] = Subset(dataset, train_idx)\n","    datasets['val'] = Subset(dataset, val_idx)\n","    return datasets\n","\n","def get_get_train_val_test(dataset, val_split=0.4):\n","  subset = split_data(dataset, val_split = val_split)\n","  eval_set = split_data(subset['val'], val_split = 0.5)\n","  \n","  train_set = subset['train']\n","  val_set = eval_set['train']\n","  test_set = eval_set['val']\n","\n","  return (train_set, val_set, test_set)\n","\n","\n","def get_accuracy(logits, labels):\n","  preds = torch.argmax(logits, axis=1)\n","  matches = preds == labels\n","  return (matches.sum(), len(labels))\n","\n","\n","def evaluate(model, test_set, batch_size, criterion, ep):\n","  test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle=True)\n","  test_iterator = tqdm(test_loader, desc = 'Eval Iteration for epoch:'+str(ep+1), ncols = 900)\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  \n","   \n","\n","  model.eval()\n","  global_step = 0\n","  total_correct = 0\n","  total_samples = 0\n","  total_loss = 0.0\n","  for step, inputs in enumerate(test_iterator):\n","      global_step +=1\n","      # if global_step > 500:\n","      #   break\n","      x = inputs[0]\n","      y = inputs[1].long()\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      logits = model(x)\n","      loss = criterion(logits, y)\n","      correct, samples = get_accuracy(logits, y)\n","      total_correct +=correct.item()\n","      total_samples +=samples\n","      total_loss +=loss\n","  # print(total_correct, total_samples)\n","  acc = total_correct / total_samples\n","  total_loss = total_loss / global_step\n","  # model.train()\n","  \n","  return (total_loss, acc)\n","\n","\n","def train(model, train_set, val_set, test_set , batch_size = 16, learning_rate = 0.03, epochs = 5, eval_steps = 10, skip_train_set = True):\n","  criterion = nn.CrossEntropyLoss()\n","\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  \n","  train_log = open(\"log/train.log\", \"w\")\n","  val_log = open(\"log/val.log\", \"w\")\n","  test_log = open(\"log/test.log\", \"w\")\n","  \n","  model = model.to(device)\n","\n","  # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","  train_loader = torch.utils.data.DataLoader(dataset= train_set, batch_size=batch_size, shuffle=True)\n","  global_step = 0\n","  for ep in tqdm(range(epochs), desc = ' Epoch Progress:', ncols=900):\n","    train_iterator = tqdm(train_loader, desc = 'Train Iteration for epoch:'+ str(ep+1), ncols=900)    \n","    for step, inputs in enumerate(train_iterator):\n","      model.train()\n","      optimizer.zero_grad()\n","\n","      global_step +=1\n","      # if global_step > 10:\n","      #   break\n","      x = inputs[0]\n","      y = inputs[1].long()\n","      x = x.to(device)\n","      y = y.to(device)\n","      logits = model(x)\n","\n","      loss = criterion(logits, y)\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      \n","    val_loss, val_accuracy = evaluate(model, val_set, batch_size, criterion, ep)\n","    val_log.write(\"Epoch = {}, validation loss =  {}, validation accuracy = {} \\n\".format(ep+1, val_loss, val_accuracy))\n","    \n","    if not skip_train_set:\n","      train_loss , train_accuracy = evaluate(model, train_set, batch_size, criterion, ep)\n","      train_log.write(\"Epoch = {}, training loss =  {}, training accuracy = {} \\n\".format(ep+1, train_loss, train_accuracy))\n","      print(\"Step = %d, training loss =  %f, training accuracy = %f\" %(global_step, train_loss, train_accuracy))\n","\n","    print(\"Step = %d, validation loss =  %f, validation accuracy = %f\" %(global_step, val_loss, val_accuracy))\n","\n","  if test_set is not None:  \n","    test_loss, test_accuracy = evaluate(model, test_set, batch_size, criterion, ep)\n","    test_log.write(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","    print(\"End of Training, test loss =  %f, test accuracy = %f\" %(test_loss, test_accuracy))\n","\n","  train_log.close()\n","  val_log.close()\n","  test_log.close()\n","\n","def main():\n","## main\n","  input_dim =  10\n","  output_classes = 1\n","  learning_rate = 0.001\n","  batch_size = 4\n","  epochs = 10\n","  eval_steps = 100\n","  ####\n","\n","\n","  train_set, val_set, test_set = None, None, None\n","  train_set = get_cifar_dataset(train = True)\n","  val_set = get_cifar_dataset(train = False)\n","\n","\n","\n","\n","  model = CNN()\n","\n","\n","\n","  train(model, train_set, val_set, test_set , batch_size = batch_size, learning_rate = learning_rate, epochs = epochs, eval_steps = eval_steps, skip_train_set = True)\n","\n","\n","if __name__ == \"__main__\":\n","  main()\n","\n","\n"],"execution_count":null,"outputs":[]}]}