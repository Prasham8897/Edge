{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_mv.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YqpvV7WFA862","executionInfo":{"status":"ok","timestamp":1605478934600,"user_tz":300,"elapsed":31206,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"82d27af8-adb0-4557-e81d-03c8e43fcecf","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JrxJuOP8BTw_"},"source":["MY_GOOGLE_DRIVE_PATH = 'My Drive/Capstone_Prasham/'\n","data_dir = ROOT + MY_GOOGLE_DRIVE_PATH + '/Edge/data/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9beMmNvBFIP","executionInfo":{"status":"ok","timestamp":1605478939025,"user_tz":300,"elapsed":306,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"aa771a1c-706c-427a-c34a-db6f0f8025ed","colab":{"base_uri":"https://localhost:8080/"}},"source":["from os.path import join\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","print(\"PROJECT_PATH: \", PROJECT_PATH)   "],"execution_count":null,"outputs":[{"output_type":"stream","text":["PROJECT_PATH:  /content/drive/My Drive/Capstone_Prasham/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fe1g4X9XBdOe","executionInfo":{"status":"ok","timestamp":1605478939810,"user_tz":300,"elapsed":367,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"f4b97bc3-09d5-476c-de54-259777bca08a","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd \"{PROJECT_PATH}\"\n","%cd \"Edge\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Capstone_Prasham\n","/content/drive/My Drive/Capstone_Prasham/Edge\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c3rr31yXB4P2"},"source":["# **Model = DNN, Data = MV data**"]},{"cell_type":"code","metadata":{"id":"yxehL6eeB5kE","executionInfo":{"status":"ok","timestamp":1605479625906,"user_tz":300,"elapsed":436,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"4c277bff-94fb-4a95-cf92-ef3c346fae7b","colab":{"base_uri":"https://localhost:8080/"}},"source":["# %%writefile train_mv.py\n","import time\n","import torch\n","import os\n","from model.dnn import DenseNeuralNet\n","from data.mv_data import MVDataset\n","import torch.nn as nn\n","from utils.util_functions import *\n","from tqdm.auto import trange, tqdm\n","import numpy as np\n","from datetime import datetime\n","\n","#Setting Random Seed\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","\n","def evaluate(model, test_set, batch_size, criterion, ep):\n","  test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle=True)\n","  test_iterator = tqdm(test_loader, desc = 'Eval Iteration for epoch:'+str(ep+1), ncols = 900)\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","  model.eval()\n","  global_step = 0\n","  total_correct = 0\n","  total_samples = 0\n","  total_loss = 0.0\n","  for step, inputs in enumerate(test_iterator):\n","      global_step +=1\n","      x = inputs[0]\n","      y = inputs[1]\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      logits = model(x)\n","      loss = criterion(logits, y)\n","      total_loss +=loss\n","\n","  total_loss = total_loss / global_step \n","  return total_loss\n","\n","\n","def train(model, train_set, val_set, test_set , batch_size = 16, learning_rate = 0.03, epochs = 5, eval_steps = 10, skip_train_set = True):\n","  criterion = nn.MSELoss()\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  model = model.to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","  \n","  time_stamp = str(datetime.now())\n","  filename = \"MV_Dataset_\" + time_stamp\n","  train_log = open(\"log/\"+ filename +\"_train.log\", \"w\")\n","  val_log = open(\"log/\"+ filename +\"_val.log\", \"w\")\n","  test_log = open(\"log/\"+ filename +\"_test.log\", \"w\")\n","  \n","  train_loader = torch.utils.data.DataLoader(dataset= train_set, batch_size=batch_size, shuffle=True)\n","  global_step = 0\n","  for ep in tqdm(range(epochs), desc = ' Epoch Progress:', ncols=900):\n","    train_iterator = tqdm(train_loader, desc = 'Train Iteration for epoch:'+ str(ep+1), ncols=900)    \n","    for step, inputs in enumerate(train_iterator):\n","      model.train()\n","      optimizer.zero_grad()\n","\n","      global_step +=1\n","      x = inputs[0]\n","      y = inputs[1]\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      logits = model(x)\n","      loss = criterion(logits, y)\n","      loss.backward()\n","      optimizer.step()\n","\n","      \n","    val_loss = evaluate(model, val_set, batch_size, criterion, ep)\n","    val_log.write(\"Epoch = {}, validation loss =  {} \\n\".format(ep+1, val_loss))\n","    \n","    if not skip_train_set:\n","      train_loss  = evaluate(model, train_set, batch_size, criterion, ep)\n","      train_log.write(\"Epoch = {}, training loss =  {} \\n\".format(ep+1, train_loss))\n","      print(\"Step = %d, training loss =  %f\" %(global_step, train_loss))\n","\n","    print(\"Step = %d, validation loss =  %f\" %(global_step, val_loss))\n","    \n","  test_loss = evaluate(model, test_set, batch_size, criterion, ep)\n","  test_log.write(\"End of training, test loss =  {}\\n\".format(test_loss))\n","  print(\"End of Training, test loss =  %f\" %(test_loss))\n","\n","  train_log.close()\n","  val_log.close()\n","  test_log.close()\n","\n","def quantization_eval_results(model_name,train_set,test_set,batch_size,criterion):\n","  results = quantization(model_name)\n","  train_loss_list = []\n","  # train_accuracy_list = []\n","  test_loss_list = []\n","  # test_accuracy_list = []\n","  for i in results[\"model_artifact\"]:\n","    train_loss = evaluate(model=i, \n","                                        test_set = train_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0) \n","    test_loss = evaluate(model=i, \n","                                        test_set = test_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0)  \n","    train_loss_list.append(train_loss.item())\n","    test_loss_list.append(test_loss.item())\n","    # train_accuracy_list.append(train_accuracy)\n","    # test_accuracy_list.append(test_accuracy)\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(train_loss, train_accuracy))\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","  results[\"train_loss\"] = train_loss_list\n","  # results[\"train_acc\"] = train_accuracy_list\n","  results[\"test_loss\"] = test_loss_list\n","  # results[\"test_acc\"] = test_accuracy_list\n","  return results\n","\n","def pruning_eval_results(model_name,train_set,test_set,batch_size,criterion):\n","  results = pruning_multiple(model_name)\n","  train_loss_list = []\n","  train_accuracy_list = []\n","  test_loss_list = []\n","  test_accuracy_list = []\n","  for i in results[\"pruned_model_artifact\"]:\n","    train_loss = evaluate(model=i,\n","                          test_set = train_set,\n","                          batch_size=batch_size, \n","                          criterion=criterion,\n","                          ep=0) \n","    test_loss = evaluate(model=i, \n","                         test_set = test_set,\n","                         batch_size=batch_size, \n","                         criterion=criterion,\n","                         ep=0)  \n","    train_loss_list.append(train_loss.item())\n","    test_loss_list.append(test_loss.item())\n","    # train_accuracy_list.append(train_accuracy)\n","    # test_accuracy_list.append(test_accuracy)\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(train_loss, train_accuracy))\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","  results[\"train_loss\"] = train_loss_list\n","  # results[\"train_acc\"] = train_accuracy_list\n","  results[\"test_loss\"] = test_loss_list\n","  # results[\"test_acc\"] = test_accuracy_list\n","  return results\n","\n","def main(train_model=True,pruning=False,quantize=False):\n","## main\n","  input_dim =  10\n","  output_classes = 1\n","  learning_rate = 0.001\n","  batch_size = 16\n","  epochs = 20\n","  eval_steps = 100\n","   ####\n","  model_dir = 'model_artifacts'\n","  model_simple_name = 'mv_simple.pt'\n","  model_complex_name = 'mv_complex.pt'\n","  ####\n","  mv_dataset = MVDataset()\n","  train_set, val_set, test_set = get_get_train_val_test(mv_dataset, \n","                                                        val_split=0.40)\n","  \n","  if(train_model):\n","\n","    print(\"-------------------------------------------------------\")\n","    print(\"Training Model: 1\")\n","    model_simple = DenseNeuralNet(input_size = input_dim, \n","                                  num_classes = output_classes,\n","                                  layers = [10],\n","                                  dropout_prob=0,\n","                                  batch_norm=False)   \n","    print(\"-------------------------------------------------------\")\n","    print(model_simple)\n","    print(\"-------------------------------------------------------\")\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model_simple.parameters(), lr=learning_rate)\n","\n","    train(model = model_simple,\n","          train_set = train_set, \n","          val_set = val_set, \n","          test_set = test_set , \n","          batch_size = batch_size, \n","          learning_rate = learning_rate, \n","          epochs = epochs, \n","          eval_steps = eval_steps,\n","          skip_train_set=False)  \n","    torch.save(model_simple, os.path.join(model_dir, model_simple_name))\n","    print(\"-------------------------------------------------------\")\n","    print(\"Training Model: 2\")\n","    model_complex = DenseNeuralNet(input_size = input_dim, \n","                                  num_classes = output_classes,\n","                                  layers = [50,100,100,50],\n","                                  dropout_prob=0.10,\n","                                  batch_norm=False)  \n","    print(\"-------------------------------------------------------\")\n","    print(model_complex)\n","    print(\"-------------------------------------------------------\")\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model_complex.parameters(), lr=learning_rate)\n","\n","    train(model = model_complex,\n","          train_set = train_set, \n","          val_set = val_set, \n","          test_set = test_set , \n","          batch_size = batch_size, \n","          learning_rate = learning_rate, \n","          epochs = epochs, \n","          eval_steps = eval_steps,\n","          skip_train_set=False)  \n","    torch.save(model_complex, os.path.join(model_dir, model_complex_name))\n","  else:\n","    if (pruning):\n","      criterion = nn.MSELoss()\n","      path_result = \"data/results/\"\n","\n","      results_simple = pruning_eval_results(model_simple_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","      results_complex = pruning_eval_results(model_complex_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","\n","      time_stamp = str(datetime.now())\n","      results_simple.to_csv(path_result + \"pruning_mv_simple_\" +time_stamp+ \".csv\")\n","      results_complex.to_csv(path_result + \"pruning_mv_complex_new_\"+time_stamp+\".csv\")\n","    if (quantize):\n","      criterion = nn.MSELoss()\n","      path_result = \"data/results/\"\n","\n","      results_simple = quantization_eval_results(model_simple_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","      results_complex = quantization_eval_results(model_complex_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","\n","      time_stamp = str(datetime.now())\n","      results_simple.to_csv(path_result + \"mv_simple_\" +time_stamp+ \".csv\")\n","      results_complex.to_csv(path_result + \"mv_complex_new_\"+time_stamp+\".csv\")\n","\n","if __name__ == \"__main__\":\n","  main(train_model=False,pruning=True,quantize=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Overwriting train_mv.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"psuzcBnjXA1Y"},"source":[""],"execution_count":null,"outputs":[]}]}