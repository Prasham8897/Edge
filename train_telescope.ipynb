{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_telescope.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YqpvV7WFA862","executionInfo":{"status":"ok","timestamp":1605478530106,"user_tz":300,"elapsed":342,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"070be1bc-4d17-4f8c-e2be-1e1663a04ad9","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive # import drive from google colab\n","\n","ROOT = \"/content/drive\"     # default location for the drive\n","print(ROOT)                 # print content of ROOT (Optional)\n","\n","drive.mount(ROOT)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JrxJuOP8BTw_","executionInfo":{"status":"ok","timestamp":1605478530936,"user_tz":300,"elapsed":333,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}}},"source":["MY_GOOGLE_DRIVE_PATH = 'My Drive/Capstone_Prasham/'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9beMmNvBFIP","executionInfo":{"status":"ok","timestamp":1605478531505,"user_tz":300,"elapsed":275,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"d786dbc2-d376-46ae-ec31-b81fb1948470","colab":{"base_uri":"https://localhost:8080/"}},"source":["from os.path import join\n","PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n","\n","print(\"PROJECT_PATH: \", PROJECT_PATH)   "],"execution_count":3,"outputs":[{"output_type":"stream","text":["PROJECT_PATH:  /content/drive/My Drive/Capstone_Prasham/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fe1g4X9XBdOe","executionInfo":{"status":"ok","timestamp":1605478532399,"user_tz":300,"elapsed":376,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"c1a1fe3f-6ec3-4b54-81a0-5815522b5033","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd \"{PROJECT_PATH}\"\n","%cd \"Edge\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Capstone_Prasham\n","/content/drive/My Drive/Capstone_Prasham/Edge\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c3rr31yXB4P2"},"source":["# **Model = DNN, Data = Telescope data**"]},{"cell_type":"code","metadata":{"id":"yxehL6eeB5kE","executionInfo":{"status":"ok","timestamp":1605478882056,"user_tz":300,"elapsed":560,"user":{"displayName":"Prasham Dhaneshbhai Sheth","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhPxMPjT3hLrfd-qp275b9YOBeRb63ufiWqNopOXw=s64","userId":"10912428145491452346"}},"outputId":"42798d11-2c30-40fd-b7d8-1895339da59f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# %%writefile train_telescope.py\n","import time\n","import torch\n","import torch.nn as nn\n","import os\n","from model.dnn import DenseNeuralNet\n","from data.telescope_data import TelescopeDataset\n","from utils.util_functions import *\n","from tqdm.auto import trange, tqdm\n","from tqdm import trange\n","from torch.utils.data import Subset\n","import numpy as np\n","from datetime import datetime\n","\n","\n","#Setting Random Seed\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","def evaluate(model, test_set, batch_size, criterion, ep):\n","  test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size = batch_size, shuffle=True)\n","  test_iterator = tqdm(test_loader, desc = 'Eval Iteration for epoch:'+str(ep+1), ncols = 900)\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  \n","  model.eval()\n","  global_step = 0\n","  total_correct = 0\n","  total_samples = 0\n","  total_loss = 0.0\n","  for step, inputs in enumerate(test_iterator):\n","      global_step +=1\n","      x = inputs[0]\n","      y = torch.squeeze(inputs[1],1).long()\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      logits = model(x)\n","      loss = criterion(logits, y)\n","      correct, samples = get_accuracy(logits, y)\n","      total_correct +=correct.item()\n","      total_samples +=samples\n","      total_loss +=loss\n","\n","  acc = total_correct / total_samples\n","  total_loss = total_loss / global_step\n","  \n","  return (total_loss, acc)\n","\n","\n","def train(model, train_set, val_set, test_set , batch_size = 16, learning_rate = 0.03, epochs = 5, eval_steps = 10, skip_train_set = True):\n","  criterion = nn.CrossEntropyLoss()\n","\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  model = model.to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","  time_stamp = str(datetime.now())\n","  filename = \"Telescope_Dataset_\" + time_stamp\n","  train_log = open(\"log/\"+ filename +\"_train.log\", \"w\")\n","  val_log = open(\"log/\"+ filename +\"_val.log\", \"w\")\n","  test_log = open(\"log/\"+ filename +\"_test.log\", \"w\")\n","  \n","\n","  train_loader = torch.utils.data.DataLoader(dataset= train_set, batch_size=batch_size, shuffle=True)\n","  global_step = 0\n","  for ep in tqdm(range(epochs), desc = ' Epoch Progress:', ncols=900):\n","    train_iterator = tqdm(train_loader, desc = 'Train Iteration for epoch:'+ str(ep+1), ncols=900)    \n","    for step, inputs in enumerate(train_iterator):\n","      model.train()\n","      model.zero_grad()\n","\n","      global_step +=1\n","      # if global_step > 10:\n","      #   break\n","      x = inputs[0]\n","      y = torch.squeeze(inputs[1],1).long()\n","      x = x.to(device)\n","      y = y.to(device)\n","\n","      logits = model(x)\n","      loss = criterion(logits, y)\n","      loss.backward()\n","      optimizer.step()\n","\n","      \n","    val_loss, val_accuracy = evaluate(model, val_set, batch_size, criterion, ep)\n","    val_log.write(\"Epoch = {}, validation loss =  {}, validation accuracy = {} \\n\".format(ep+1, val_loss, val_accuracy))\n","    \n","    if not skip_train_set:\n","      train_loss , train_accuracy = evaluate(model, train_set, batch_size, criterion, ep)\n","      train_log.write(\"Epoch = {}, training loss =  {}, training accuracy = {} \\n\".format(ep+1, train_loss, train_accuracy))\n","      print(\"Step = %d, training loss =  %f, training accuracy = %f\" %(global_step, train_loss, train_accuracy))\n","\n","    print(\"Step = %d, validation loss =  %f, validation accuracy = %f\" %(global_step, val_loss, val_accuracy))\n","    \n","  test_loss, test_accuracy = evaluate(model, test_set, batch_size, criterion, ep)\n","  test_log.write(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","  print(\"End of Training, test loss =  %f, test accuracy = %f\" %(test_loss, test_accuracy))\n","\n","  train_log.close()\n","  val_log.close()\n","  test_log.close()\n","\n","def quantization_eval_results(model_name,train_set,test_set,batch_size,criterion):\n","  results = quantization(model_name)\n","  train_loss_list = []\n","  train_accuracy_list = []\n","  test_loss_list = []\n","  test_accuracy_list = []\n","  for i in results[\"model_artifact\"]:\n","    train_loss, train_accuracy = evaluate(model=i, \n","                                        test_set = train_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0) \n","    test_loss, test_accuracy = evaluate(model=i, \n","                                        test_set = test_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0)  \n","    train_loss_list.append(train_loss.item())\n","    test_loss_list.append(test_loss.item())\n","    train_accuracy_list.append(train_accuracy)\n","    test_accuracy_list.append(test_accuracy)\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(train_loss, train_accuracy))\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","  results[\"train_loss\"] = train_loss_list\n","  results[\"train_acc\"] = train_accuracy_list\n","  results[\"test_loss\"] = test_loss_list\n","  results[\"test_acc\"] = test_accuracy_list\n","  return results\n","\n","def pruning_eval_results(model_name,train_set,test_set,batch_size,criterion):\n","  results = pruning_multiple(model_name)\n","  train_loss_list = []\n","  train_accuracy_list = []\n","  test_loss_list = []\n","  test_accuracy_list = []\n","  for i in results[\"pruned_model_artifact\"]:\n","    train_loss, train_accuracy = evaluate(model=i, \n","                                        test_set = train_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0) \n","    test_loss, test_accuracy = evaluate(model=i, \n","                                        test_set = test_set,\n","                                        batch_size=batch_size, \n","                                        criterion=criterion,\n","                                        ep=0)  \n","    train_loss_list.append(train_loss.item())\n","    test_loss_list.append(test_loss.item())\n","    train_accuracy_list.append(train_accuracy)\n","    test_accuracy_list.append(test_accuracy)\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(train_loss, train_accuracy))\n","    # print(\"End of training, test loss =  {}, test accuracy = {} \\n\".format(test_loss, test_accuracy))\n","  results[\"train_loss\"] = train_loss_list\n","  results[\"train_acc\"] = train_accuracy_list\n","  results[\"test_loss\"] = test_loss_list\n","  results[\"test_acc\"] = test_accuracy_list\n","  return results\n","def main(train_model=True,pruning=False,quantize=False):\n","## main\n","  input_dim =  10\n","  output_classes = 2\n","  learning_rate = 0.001\n","  batch_size = 16\n","  epochs = 10\n","  eval_steps = 100\n","  ####\n","  model_dir = 'model_artifacts'\n","  model_simple_name = 'telescope_simple.pt'\n","  model_complex_name = 'telescope_complex.pt'\n","  ####\n","  telescope_dataset = TelescopeDataset()\n","  train_set, val_set, test_set = get_get_train_val_test(telescope_dataset, \n","                                                        val_split=0.40)\n","  if(train_model):\n","    print(\"-------------------------------------------------------\")\n","    print(\"Training Model: 1\")\n","    model_simple = DenseNeuralNet(input_size = input_dim, \n","                                  num_classes = output_classes,\n","                                  layers = [10],\n","                                  dropout_prob=0,\n","                                  batch_norm=False)   \n","    print(\"-------------------------------------------------------\")\n","    print(model_simple)\n","    print(\"-------------------------------------------------------\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model_simple.parameters(), lr=learning_rate)\n","\n","    train(model = model_simple,\n","          train_set = train_set, \n","          val_set = val_set, \n","          test_set = test_set , \n","          batch_size = batch_size, \n","          learning_rate = learning_rate, \n","          epochs = epochs, \n","          eval_steps = eval_steps,\n","          skip_train_set=False)  \n","    torch.save(model_simple, os.path.join(model_dir, model_simple_name))\n","    print(\"-------------------------------------------------------\")\n","    print(\"Training Model: 2\")\n","    model_complex = DenseNeuralNet(input_size = input_dim, \n","                                  num_classes = output_classes,\n","                                  layers = [20,40,60,30],\n","                                  dropout_prob=0.10,\n","                                  batch_norm=False)  \n","    print(\"-------------------------------------------------------\")\n","    print(model_complex)\n","    print(\"-------------------------------------------------------\")\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model_complex.parameters(), lr=learning_rate)\n","\n","    train(model = model_complex,\n","          train_set = train_set, \n","          val_set = val_set, \n","          test_set = test_set , \n","          batch_size = batch_size, \n","          learning_rate = learning_rate, \n","          epochs = epochs, \n","          eval_steps = eval_steps,\n","          skip_train_set=False)  \n","    torch.save(model_complex, os.path.join(model_dir, model_complex_name))\n","  \n","\n","  else:\n","    if (pruning):\n","      criterion = nn.CrossEntropyLoss()\n","\n","      path_result = \"data/results/\"\n","\n","      results_simple = pruning_eval_results(model_simple_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","      results_complex = pruning_eval_results(model_complex_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","\n","      time_stamp = str(datetime.now())\n","      results_simple.to_csv(path_result + \"Pruning_telescope_simple_\" +time_stamp+ \".csv\")\n","      results_complex.to_csv(path_result + \"Pruning_telescope_complex_new_\"+time_stamp+\".csv\")\n","    if (quantize):\n","      criterion = nn.CrossEntropyLoss()\n","\n","      path_result = \"data/results/\"\n","\n","      results_simple = quantization_eval_results(model_simple_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","      results_complex = quantization_eval_results(model_complex_name,train_set=train_set,test_set=test_set,batch_size=batch_size,criterion=criterion)\n","\n","      time_stamp = str(datetime.now())\n","      results_simple.to_csv(path_result + \"telescope_simple_\" +time_stamp+ \".csv\")\n","      results_complex.to_csv(path_result + \"telescope_complex_new_\"+time_stamp+\".csv\")\n","if __name__ == \"__main__\":\n","  main(train_model=False,pruning=True,quantize=False)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Overwriting train_telescope.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"duZMg_zm98Mm"},"source":[""],"execution_count":null,"outputs":[]}]}